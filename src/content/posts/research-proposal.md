---
title: Fast and Versatile Vocabulary Test
date: 2025-01-14
---

[notes](notes-meeting-24-03)

# Introduction
## Background and Motivation
Adaptive learning is an software design technic designed around the maturing technology of recomender systems to provide personalized and optimized learning eperience to their users. The recomender system works based on psychometric assessment relevent to the domain the software aims to teach, but no such metric exists in the context of second language (L2) teaching. Today, most recomender systems are implemented in contexts where this metric is readily available, being either attention, in the case of social media, or clients spending, in the case of marketplace. But how to optimize for a construct as evasive as fluency? How to test quickly enough so that a test can be retaken on a regular basis? How to guaranty that the results are precise enough to identify the small progress made in a short amount of time, and thus rate the pedagogical value of a material?
While specialists are still debating the nature of the constructs that would constitute a definition of fluency, the present work does not intend to build a complete language proficiency test, nor does it even support the idea that such a test can ever be created. This does not however mean that progress in L2 acquisition cannot be measured efficiently. Indeed, research have found that mere vocabulary tests based on linguistic decision tasks (LDT) yield results correlating with a range of other skills that are usually tested in larger language tests, at different proficiency levels. These tests also have the strength of taking only a few minutes to administer. The rational behind these tests lies in the fact that identification is the first stage of acquisition. On the other hand, acquiring a new word is an atomic step in a never-ending journey, even in native speakers, making it an excelent psychometric base.  
But these tests also have caveats limiting their scalability. First, they rely on lists of handcrafted pseudo-words, the creation of which is time-consuming and require expertise in the language in which the pseudo-words are supposed to be plausible. Second, once the words and pseudo-words are consituted, they are tested in preliminary studies, which is too resource intensive for non-WEIRD environment. This study aims to address these two challenges, and proposes a framework to evaluate the value of the psychometric results gathered.

## Aims and Objective
The final aim of this study is to provide a metric able to track the speed of progress of L2 learners at any stage of their learning journey, with the prospect that this metric could help to optimize adaptative language learning systems. To this end, this dissertation develops and evaluate solutions that enables both the horizontal and vertical scalability of binary-choice vocabulary tests. The vertical scalability is defined as the ability to increase the capacity of a signle tests. The horizontal scalability is defined as the ability to scale up the range of languages that can be tested. To validate the achievement of these goals, the following concrete objectives are fixed: 
- Vertical scalability: run a large scale online study and optain concistent results when repeatedly testing the same people.
- Horizontal scalability: achieving the above objective with a test for at least one low resource language.
- Analysis: develop a methodology to assess the relevence of the technical decisions and explore enventual alternative ways to treat the results of a binary-choice vocabulary test in the context of adaptative learning solutions.

# Research Question, Hypothesis and Methodology
## Research Question
If the dissertation methodology and implications span all accross cognitive sciences, the research question is more restricted to the field of applied linguistics and psycholinguistics. By attempting to clarify the nature of vocabulary knowledge accross a speaking population, it seeks to evaluate the relevance technical solutions proposed in the dissertation.

 Does the difficulty distribution of word items in a speaking population support the ideau of a continuous or segmented progression in vocabulary level?

This research question leads to the two following hypothesis.

## Hypothesis
In the first case, we postulate that, as words have different difficulty levels, the ability to recognize some items of a given difficulty range uniformally increase the chances of recognizing words of a similar difficulty level. This idea of uniform progression in the vocabulary knowledge, where speakers learn words in roughly the same order, analoguously to the Zipfian progression, is what we call the linear progression hypothesis or hypothesis 1.  
In the second case, we assume that the speakers of a language are segmented into groups among which the distribution of the chances to recognize some groups of items varies at similar vocabulary levels. In this context, a large portion of the vocabulary tested seem to share the same overall chances of being recognised, but for some reason, the tests results would show that groups of people would only be able to recognise domain-specific groups of words. In this case the description of the level of the subjects should reflect the diversity of the domains of the vocabulary known more than its difficulty and the difficulty distribution of the vocabulary would form clusters instead of display a uniform progression. We name this hypothesis the clusters hypothesis, or hypothesis 0.
If the second hypothesis turns out to be true, it would show the need to develop a multidimensional representation of both the expertise of a speaker and the characteristics of a word, instead of the one dimensional, one-size-fits-all, vocabulary level score developed in this dissertation to accurately predict the results of a task and build a recomender system for knowledge analog to vocabulary.

## Methodology
To answer the research question, the test must be scaled up in order to gather data on large samples of vocabulary, if not an entire dictionary of the language to be investigated. With these large sets of words, an equally large set of language-specific pseudo-words have to be created. This is the first obstacle addressed in this section. A second issue discussed here is the need to assess the difficulty value of these large sets of items, together with the testees themselves, without preliminary studies.
### Generating the Pseudo-words
A large set of real words can be easily sourced by K-sampling any dictionary, but constituting a large set of orthographically and phonotactically correct words can be challenging. Handcrafting the pseudo-words is not an option for two reasons, first, the process would require expertise in the language in which one desires to create plausible, yet non-real words. Second, even if this expertise were available, the amount of items to produce is requires resources beyond what most languages around the world have available. This is why the pseudo-words must be programatically generated. To this end, the priviledged method is to treat treat words as hidden Markov chain models (MCM) of n-gram of characters (usually bigram or threegrams). This technique has the advantage to be easy to understand and to implement with limited programing skills, but it has two inconvenients that must be taken into account. First, this technique does not allow the creation of really short, but plausible words, less than 3 or 4 characters long, this could help test takers to identify real words or force the test maker to remove shorter words from the real words list. The second problem is more incidious. Some languages have what is called phonotactic long distance relationships, where non adjacent parts of the words interract together, like vowel harmony in Turkish. In Welsh, some letters or groups of letters ar only "legal" in some precise parts of a word, relative to the stressed sylable or the end of the word. As HMCM cannot account for these subtleties, the words used in the test will be generated by recurent neural networks (RNN) trained to predict the next character of the real words used in the test.

### Rating the Results and Item Selection
The second obstacle is the calibration of the results. Previous tests rely on preliminary studies to select items that can differenciate different levels of proficiency. The format of a dissertation does not allow for preliminary studies, instead, the test proposed here continuously updates the level of difficulty of the word items to assess to level of the test takers, while the level of the test takers is in turn used to calibrate the difficulty level of the items. This problem of co-calibration of the difficulty and skill levels was solved independently several times in history and led to the creation of item response theory (IRT) of which the one parameter model, also called the Rasch model is the most famous. This system can easily be implemented to assess both the test takers level and the items difficulty level.
Another divergeance from the format of other similar vocabulary tests is that all the word items are not intended to be tested. The test should not take more than five minutes to administer, but the volume of available items in the thousands. Furthermore, if a test taker only knows a small fraction of these items, a random selection would require a large number of answers, potentially hundreds per testing session, before gathering statistically meaningful results. In order to stay relevant for all types of language learners, the selection of the items has to display some degree of adaptativity through the implementation of an item recomendater system. For this, the difficulty rating of the items and the skill rating of the test takers must be updated in real-time. This is what the Elo rating system does, which is a system based on the same equations, although invented independently, that uses slightly different parameters to make the ratings human-readable. For a comparision of the Elo rating system with other IRT algorithms, refer to Pel√†nek (2016).  
Since this methodology is work by giving ratings on the same scale to both test items and test takers, we can use the descrepency between the test taker rating distribution and the items rating distribution to validate the one of the two hypothesis, and thus answer the research question. If, in the distribution of words by difficulty, a cluster of words sharing have the same level at level where there are little word items, it would mean that the test takers in this cluster can systematically recognize only a part of the words with higher rating, but not all of it, which in turn means that the notion of word difficulty would be partially inadapted to the context of vocabulary testing, invalidating the hypothesis 1. 

### Evaluate the Results
Once the results are gathered, the hypothesis is tested by calculating the mean silhouette scores, of different K-means clusterings. If the value of the silhouette score stays low for different values, then the hypothesis 0 is invalidated and the idea of a continuous progression in vocabulary knowledge supported. If a strong silhouette score can be identified for some value of K, then the hypothesis 1 is invaludated and the need for another, multidimensional representation of vocabulary difficulty and knowledge is supported.

# Limitations
No differenciation between native and non native speakers, between backgrounds in general.
Despite the testing process itself can be quick enough to administer, there is no guaranty that it will capture progressions made over the span of a week.
Not truly scalable to all the populations without the creation of a oral version of the test, which would bring its own problem. 
But non of these problems invalidates the results of the test. 
Ethical considerations: misinterpretation or misuse of the test results.

# Discussions
Once the data are gathered, the goal would obviously to create a model as precise as possible of vocabulary knowledge. In a technical language, this means being able to predict the chance of a word to be recognised from as little vocabulary items as possible. This can be achieve with neural networks. In fact, such modelisation allows to predict the next most likely words to be learned by a given learner. In this sens, a practical use of the project would be to create a vocabulary list generator based on a test result. Going forward, the modelisation work could even fo further by evaluating have a vocabulary list improve the scores of the learner, thus learning to optimise for the few items that maximise vocabulary knowledge growth.