---
title: Fast and Versatile Vocabulary Test
date: 2025-01-14
---
I'w wneud: ffeindio Cwestiwn Ywmchwil.
[notes](notes-meeting-24-03)
# 1 Introduction (320 words)
A recurrent critique levelled at psycholinguistics is its focus on predominantly educated Western populations, most likely English-speaking, and the risk of wrongly be generalizing potentially biased findings. Its status as lingua franca make English the main focus even on L2 research (citation_needed maybe Paul Nation) and little account is given in the litterature on lower resource languages, apart from code-switching, which is in a way an extension of the research on the "stronger" languages, rather than studies on the other languages. For example, if a correlation is found between a higher proficiency in L2 and in L1 where English is one of the two langauges, can we predict that all bilinguals are likely to speak better than average in their L1? The sociolinguistic background of people from developed countries learning English, or vice versa may well bias conclusions on this matter. In looking for such correlations, it is essential to gather data from as many language pairs as possible to offset potential bias. The experimental design below presents a protocol to answer the following question: "Do above-than-average proficiencies in L2 and L1 correlate?". 
The protocol introduced below thus attempts at providing some standard for language proficiency assessment suitable for low resource languages. It does so by drawing largely from already established and tested protocols in the field, but optimizing each steps upstrean and during the experiment to aleviate the gap in resources most languages around the world experience compared with English (time, linguistic expertise, people to run preliminary studies etc...). However minimalist the design is made by these constrains, the test itself does not yield minimalist nor aproximate results. The relatively simple aspect of the design also allows for modifications an tailoring to specific needs and incremental improvements, thus making it a basis for a new approach in psycholinguistic research in general. Especially when testing skill variations through time, the dynamic aspect of its underlying framework may be able to find application behind mere vocabulary testing.

# 2 Litterature Review (870 words)
As studies have found vocabulary knowledge to be a good indicator of general language proficiency (Lemhöfer and Broersma 2012), this section will give an account of the different approaches used to assess vocabulary mastery through history, comparing their strengths and weaknesses regarding the requirements of building a test scalable for lower-resource language, yet revalizing with those used for higher-resource languages.

## 2.1 Defining Vocabulary Mastery
As pointed out by Brysbaert et al. (2016), the measure of vocabulary size will depend on the definition of what a word is (alphabetical type, lemma or word family) and the criteria used to validate that a tested word is knows. Should the word be recognised, understood, translated or described with other words? Should a mastery of all the semantic aspects of word be displayed for that word to be truly understood? And how to deal with homonyms? This section scrutinizes the different aproaches to this problem.

### 2.1.1 Cross Product of Systematic Sampling of Dictionaries
Hartmann (1946) and Goulden et al. (1990) used a cross product based on systematic sampling from dictionaries and the size of the dictionary themselves to deduce vocabulary length. In Hartman, the testees were asked to describe the word without time limits (Brysbaert et al. 2016). In the other study, they were asked if they recognised the words. Significantly different result were found by the two studies: 215 000 by Hartmann and 17 200 by Goulden. Although Goulden's study excluded proper nouns, derived words, and compounds (ibid.), the threshold for word knowledge was also arguably lower, which would indicate a small difference between the ability to recognise words and describe them, at least partially. Relying on self reported recognition of the words is however trustworthy as long as the testees don't have an interest in lying. The best way to make sure that the data gathered by a test are valid is to make it dificult to cheat.

### 2.1.2 Vocabulary Assessment in Intelligence Tests
Because they face the same problem of having to be non-falsifiable, IQ tests came up with their own protocoles to assess vocabulary mastery:
- Wechsler Intelligence Scales:
	- task: giving synonyms of words, finding semantic connection between words
	- good because: it does not rely on CEFR levels, it place examinees on a normal scale for their age which is more objective 
	- bad because it is a lot of preparation for building the test, it is hard to translate to other languages, it needs to be tested in order to map the normal distribution to an index, and it relies on a professional to test people, assessing the validity of the answer, which is prone to the subjective assessment of the psychologist... the same problem as commercial tests.

- Peabody Scale:
	- task: point one image on a matrix of four when earing a word
	- good because it does not rely on writing and the assessment is more objective, less prone to the psychologist judgement, it is fearly easy to translate, potentially easy to scale
	- bad because the precision of the result is not great with a limited amount of images/words sets, as for Wechsler's scale, it needs a preliminary study to get a sense of what an expected normal result is. As we shall see later, working out the expectation is key here.

- Meara's work and LexTale: A first improvement comes from Paul Meara's in vocabulary testing, using pseudo words and asking whether the testee recognise them.
	- pseudo-words and word recognition to test ranges of words ordered by frequency
	- good because tested, minimalist, LexTale is really fast, usually less than five minutes to complete, and sort testees between tree CEFR levels (B2 to C2), which shows well that the 4 or more hours sent in most commercial tests are not about testing fluency, but also, say, the ability to pass hours long exams or competitions. Meara's work also shows that the size of individuals' vocabulary size, for a given age and language, is proportional to their proficiency in the language, regardless of whether the language is L1 or L2. This almost validates the protocols aiming to answer the question "how many words do people know" as possible proficiency tests. This is why we will have a look at those protocols too.
	- inconvenient because relying on curated lists of words and non-words

Attempting to automate the workflow that would produce tests without spending time currating the vocabulary list we can think of using a list of words ranked by frequency, but this design comes with significant flaws for our stated goals:
- the spaghetti paradox, the best known words are not necessarly the most frequent, and every corpus has its own frequency distribution.
- The pseudo-words still need to be currated, wich is impractical in the context where preliminary study are made less relevant by the sheer scarcity of the language speakers.
- Even making a frequency list can be challenging depending on the language, some languages simply don't have good lemmatizers, nor even enough accessible digitalized texts!

This impracticality of making preliminary studies only leave us with one choice: reusing the data from the test to assess the items difficulty, and deduce the relative vocabulary level of the testees. Pelánek (2016) informs us of a few facts about item selection:
- a random selection of item is good (precise), yet it is impractical as the number of items becomes larger because more items need to be tested before getting meaningful data
- then we can use the proportion of correct answers: items are ordered in a scale from the most recognised to the less recognised and assessees are ordered in terms of the same ratio. We can then give people a score between 0 (no word recognised) and 1 (all words recognised). The problem is that this does not work well to study the evolution through time, (the previous bad answers penalise the future score, even though the assessees become better), this leads to requiring to restart the test every time, which is not efficient.
- To manage small adjustments, we can use the Elo rating system. => displegañ ar sistem
- Pelánek (2016) shows that despite the ordering, some level of randomness is benefitial to obtain a precise result faster. The advantage of the Elo system over the proportion of correct answers is that Elo system is based on statistcal predictions of the outcome, so using items need to have the same rating as the examinees to give an accurate account of their level.

# ## 2.2 Interpreting the Results of the Task (148)
A prominant problem in adapting vocabulary tests to new languages appears to come from the calibration and the calculation of the results. Considering the requirements of this study, the calibration phase also poses a problem of available resources; running a preliminary study for each language to select the items requires time, money and an available representative sample of the speaking populations which is unrealistic for non-LOL languages. This is why this paper presents the idea of making the preliminary study "inside" the main study, as the study goes on, using a light implementation of Item Response Theory (IRT).
If the term was coined by F.M. Lord (1980), it is Georg Rasch, working on tests data from the Danish military, who first published the concept of extracting the difficulty of the items by adjusting this difficulty score based on subtracting predictions from the actual outcome of the evaluation (Rasch \[1960] 1980). Independently invented by Ford, this methodology is now widely implemented in standardized tests such as the SAT (Steinberg 2000).  
Based on the same principles, but adapted to a dynamic context, where the difficulty of the tasks and the level of the subjects vary in time is the Elo Rating System (ERS), also independently invented at the same period to rate chess players (Elo 1961), it can be seen as the simplest algorithmic implementation of the one-parameter model (1PL) of IRT, with a Θ of 1.  
A study conduced on real data (Wauters et al. 2012) showed that IRT, the proportion of correct answers (of the items, not the testees) and the ERS all accuratly predicted the difficulty of the items. However, a study based on simulated data (Pelànek 2016) showed that the proportion of correct answers did not work as well when items are not randomly selected. This is logical, as the goal of adaptative selection is to achieve a given success ratio (ie. 50%, 80%). In the case of vocabulary testing, the large amount of word items requires an adaptative selection to obtain sensible data after a reasonable number trials, especially at a lower level. This requirement for adaptativity also disqualifies other IRT algorithms, as they are too computationally intensive to update the difficulty ratings in real time. Surprizingly, some degree of randomness would also benefit the ERS (ibid.).
## 3.1 Generating the Pseudo-Word (254)
Different languages use different letters and follow different phonotactic rules. Everytime a dictionary is added to the system, an equivalent set of non-words is generated. A solution proposed in the sofware UniPseudo (New et al. 2023) propose to make Markov chains of n-grams of set of words of the same length. 2-grams are however problematic, even in pemisive languages like English. For instance, "please" and "allied" give "pl" and "ll" 2-grams for the positions 1 and 2 respectively, which would allow a phonotactically invalid pseudo-word "pllied" to be generated. Using 3-grams seems more reasonable, yet, as Meara points out (2012) some phonotactically legal words may still be highly unlikely. This is why this protocol adds a layer sorting the pseudo-words by their average Levenshtein distance (Levenshtein 1965) from the sets of words they were generated from. Instead of using a Markov chain, simply chaining all the possible combinations of 3-grams extracted from sets of words allows more choices to select from. Applying this technique on 600 hundred Welsh words from a Hunspell dictionary, one can expect more than 7000 pseudowords to be generated, which still leaves more that 6000 strings after removing the actual words. Artefacts from the web-sourced dictionary can then be corrected by selecting the 600 "most Welsh" six-characters-long pseudo-words.  
A better generation strategy, allowing to create shorter words, would categorize the different groups of letters by looking for minimal pairs of n-grams, then deducing the rules of cohabitation between these different groups. But for now it is only one of those hypothetical incremental improvements mentioned in the introduction.  

# 5 Limitations (229 = 2 265)
Some pseudo-words might be alternative orthographs or inflected versions of real words, this can be a problem in morphologically rich languages. To offset this problem, the pseudo-words generator could generenate inflected versions of the words using Hunspell dictionaries and remove them from the pseudo-words candidates. The inverse problem may happen, real words being wrongfully added to web dictionaries of low-resources languages often happens. Both problems would be dealt in the same way by the system: unrecognised words and recognised non-words scores would go so high in rating that a "wrong answer" would uniformly and slightly penalize the users' scoring, if not at all. As pseudo-words are expected to be low in ratings; if some are going up beyond a certain threshold, the system could confidently accept them as real words, without the need for expertise in the language.  
As the task is dependent on reading skill, while most languages in the world are hardly ever written, a vocal version of the test could be added, by crowdsourcing the voices of the best rated users for a given dictionary. This would also be valuable for dyslexic and blind users.
Another risk is to see people trying to optimize their vocabulary recognition skills, like Scrable players, people could start reading or learning new words... the best way to handle this would be to make it a marker of the test's success.  


# Applications and Perspectives
Use IRT to analyze the distribution of vocabulary accross different population, be used as a main language progression meter in other psycholinguistic experiments.
Adapt to small groups.  
Use recordings instead of written words, so that the experiment can be extended to illiterate populations and languages not using phonological alphabets.

# References



